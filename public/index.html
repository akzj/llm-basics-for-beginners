<!doctype html><html lang=en dir=auto data-theme=auto><head><meta name=generator content="Hugo 0.147.0"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>LLM Basics for Beginners</title>
<meta name=description content><meta name=author content><link rel=canonical href=https://akzj.github.io/llm-basics-for-beginners/><link crossorigin=anonymous href=/llm-basics-for-beginners/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css integrity="sha256-NDzEgLn/yPBMy+XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as=style><link rel=icon href=https://akzj.github.io/llm-basics-for-beginners/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://akzj.github.io/llm-basics-for-beginners/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://akzj.github.io/llm-basics-for-beginners/favicon-32x32.png><link rel=apple-touch-icon href=https://akzj.github.io/llm-basics-for-beginners/apple-touch-icon.png><link rel=mask-icon href=https://akzj.github.io/llm-basics-for-beginners/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://akzj.github.io/llm-basics-for-beginners/index.xml title=rss><link rel=alternate hreflang=en href=https://akzj.github.io/llm-basics-for-beginners/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=/llm-basics-for-beginners/css/custom.min.7d2ee8ecd3b435c2f737e79952e8b22b4022ee7ba372f20c8c44f3a2792a885f.css integrity="sha256-fS7o7NO0NcL3N+eZUuiyK0Ai7nujcvIMjETzonkqiF8="><script defer type=text/javascript id=MathJax-script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js?config=TeX-MML-AM_CHTML"></script><script defer type=text/javascript>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0,packages:["base","ams","noerrors","noundefined"]},options:{skipHtmlTags:["script","noscript","style","textarea","pre"],enableMenu:!1},loader:{load:["[tex]/ams","[tex]/noerrors","[tex]/noundefined"]}}</script><meta property="og:url" content="https://akzj.github.io/llm-basics-for-beginners/"><meta property="og:site_name" content="LLM Basics for Beginners"><meta property="og:title" content="首页"><meta property="og:description" content="LLM Basics for Beginners 欢迎来到 LLM 基础知识学习网站 这里是一个面向初学者的大语言模型（LLM）基础知识学习平台。我们将从 Transformer 架构的核心思想出发，逐步学习 LLM 背后的数学原理和算法知识，帮助你建立起对现代 AI 语言模型的深入理解。
Transformer 核心思想 Transformer 架构是现代大语言模型的基础，它彻底改变了自然语言处理领域。其核心思想是自注意力机制（Self-Attention），这使得模型能够同时考虑输入序列中所有位置的信息，捕捉长距离依赖关系。
Transformer 架构的主要特点 注意力机制：能够自动学习输入序列中不同位置之间的依赖关系 并行计算：相比循环神经网络（RNN），Transformer 可以更高效地进行并行计算 层叠结构：通过堆叠多个注意力层和前馈神经网络层，模型可以学习到更复杂的表示 位置编码：为序列添加位置信息，弥补注意力机制本身不包含位置信息的不足 Transformer 架构的核心组件包括：
Scaled Dot-Product Attention：基础注意力计算单元 Multi-Head Attention：通过多个注意力头学习不同方面的依赖关系 Feed-Forward Networks：对每个位置的表示进行非线性变换 Layer Normalization：稳定训练过程，加速收敛 LLM 中使用的数据算法 大语言模型的实现依赖于一系列基础数学概念和算法。这些知识是理解 LLM 工作原理的关键：
1. 注意力机制相关算法 注意力机制是 Transformer 的核心，它的计算涉及到：
Scaled Dot-Product Attention $$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$
Multi-Head Attention $$ \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \text{head}_2, \dots, \text{head}_h)W^O $$
2. 激活函数与概率分布 Softmax 函数：将向量转换为概率分布，是语言生成的关键 $$ \text{softmax}(x)i = \frac{e^{x_i}}{\sum{j=1}^n e^{x_j}} $$"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="首页"><meta name=twitter:description content><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"LLM Basics for Beginners","url":"https://akzj.github.io/llm-basics-for-beginners/","description":"","logo":"https://akzj.github.io/llm-basics-for-beginners/favicon.ico","sameAs":[]}</script></head><body class=list id=top><header class=header><nav class=nav><div class=logo><a href=https://akzj.github.io/llm-basics-for-beginners/ accesskey=h title="LLM Basics for Beginners (Alt + H)">LLM Basics for Beginners</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://akzj.github.io/llm-basics-for-beginners/ title=首页><span class=active>首页</span></a></li><li><a href=https://akzj.github.io/llm-basics-for-beginners/matrix_basics/ title=矩阵基础><span>矩阵基础</span></a></li><li><a href=https://akzj.github.io/llm-basics-for-beginners/about/ title=关于><span>关于</span></a></li><li><a href=https://akzj.github.io/llm-basics-for-beginners/softmax/ title=Softmax函数><span>Softmax函数</span></a></li></ul></nav></header><main class=main><div class=post-content><h1 id=llm-basics-for-beginners>LLM Basics for Beginners<a hidden class=anchor aria-hidden=true href=#llm-basics-for-beginners>#</a></h1><h2 id=欢迎来到-llm-基础知识学习网站>欢迎来到 LLM 基础知识学习网站<a hidden class=anchor aria-hidden=true href=#欢迎来到-llm-基础知识学习网站>#</a></h2><p>这里是一个面向初学者的大语言模型（LLM）基础知识学习平台。我们将从 Transformer 架构的核心思想出发，逐步学习 LLM 背后的数学原理和算法知识，帮助你建立起对现代 AI 语言模型的深入理解。</p><h2 id=transformer-核心思想>Transformer 核心思想<a hidden class=anchor aria-hidden=true href=#transformer-核心思想>#</a></h2><p>Transformer 架构是现代大语言模型的基础，它彻底改变了自然语言处理领域。其核心思想是<strong>自注意力机制</strong>（Self-Attention），这使得模型能够同时考虑输入序列中所有位置的信息，捕捉长距离依赖关系。</p><h3 id=transformer-架构的主要特点>Transformer 架构的主要特点<a hidden class=anchor aria-hidden=true href=#transformer-架构的主要特点>#</a></h3><ol><li><strong>注意力机制</strong>：能够自动学习输入序列中不同位置之间的依赖关系</li><li><strong>并行计算</strong>：相比循环神经网络（RNN），Transformer 可以更高效地进行并行计算</li><li><strong>层叠结构</strong>：通过堆叠多个注意力层和前馈神经网络层，模型可以学习到更复杂的表示</li><li><strong>位置编码</strong>：为序列添加位置信息，弥补注意力机制本身不包含位置信息的不足</li></ol><p>Transformer 架构的核心组件包括：</p><ul><li><strong>Scaled Dot-Product Attention</strong>：基础注意力计算单元</li><li><strong>Multi-Head Attention</strong>：通过多个注意力头学习不同方面的依赖关系</li><li><strong>Feed-Forward Networks</strong>：对每个位置的表示进行非线性变换</li><li><strong>Layer Normalization</strong>：稳定训练过程，加速收敛</li></ul><h2 id=llm-中使用的数据算法>LLM 中使用的数据算法<a hidden class=anchor aria-hidden=true href=#llm-中使用的数据算法>#</a></h2><p>大语言模型的实现依赖于一系列基础数学概念和算法。这些知识是理解 LLM 工作原理的关键：</p><h3 id=1-注意力机制相关算法>1. 注意力机制相关算法<a hidden class=anchor aria-hidden=true href=#1-注意力机制相关算法>#</a></h3><p>注意力机制是 Transformer 的核心，它的计算涉及到：</p><p><strong>Scaled Dot-Product Attention</strong>
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$</p><p><strong>Multi-Head Attention</strong>
$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \text{head}_2, \dots, \text{head}_h)W^O
$$</p><h3 id=2-激活函数与概率分布>2. 激活函数与概率分布<a hidden class=anchor aria-hidden=true href=#2-激活函数与概率分布>#</a></h3><ul><li><p><strong>Softmax 函数</strong>：将向量转换为概率分布，是语言生成的关键
$$
\text{softmax}(x)<em>i = \frac{e^{x_i}}{\sum</em>{j=1}^n e^{x_j}}
$$</p></li><li><p><strong>GeLU 函数</strong>：现代 LLM 中常用的激活函数，比传统的 ReLU 具有更好的性能
$$
\text{GeLU}(x) = \frac{1}{2}x\left(1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right)
$$</p></li></ul><h3 id=3-线性代数基础>3. 线性代数基础<a hidden class=anchor aria-hidden=true href=#3-线性代数基础>#</a></h3><p>线性代数是 LLM 的数学基石，主要涉及：</p><ul><li><strong>矩阵运算</strong>：矩阵乘法、转置、逆矩阵等</li><li><strong>特征值与特征向量</strong>：用于理解线性变换的本质</li><li><strong>奇异值分解 (SVD)</strong>：矩阵分解的重要方法，用于降维和特征提取</li><li><strong>梯度下降</strong>：模型训练的核心优化算法</li></ul><h3 id=4-正则化与优化>4. 正则化与优化<a hidden class=anchor aria-hidden=true href=#4-正则化与优化>#</a></h3><ul><li><p><strong>Layer Normalization</strong>：稳定训练过程
$$
\text{LayerNorm}(x) = \alpha \odot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
$$</p></li><li><p><strong>Adam 优化器</strong>：结合了动量和自适应学习率的优化算法</p></li></ul><h2 id=展开学习从基础到进阶>展开学习：从基础到进阶<a hidden class=anchor aria-hidden=true href=#展开学习从基础到进阶>#</a></h2><p>我们将按照知识的依赖关系，为你提供系统化的学习路径：</p><h3 id=数学基础篇>数学基础篇<a hidden class=anchor aria-hidden=true href=#数学基础篇>#</a></h3><h4 id=矩阵基础知识><a href=https://akzj.github.io/llm-basics-for-beginners/matrix_basics/>矩阵基础知识</a><a hidden class=anchor aria-hidden=true href=#矩阵基础知识>#</a></h4><p>矩阵是 LLM 中最基本的数据结构，理解矩阵运算对于掌握 LLM 的工作原理至关重要。</p><p><strong>学习内容</strong>：</p><ul><li>矩阵的定义与表示</li><li>基本矩阵运算（加法、乘法、转置等）</li><li>特殊矩阵（单位矩阵、对称矩阵、正交矩阵等）</li><li>矩阵的应用场景</li></ul><h4 id=softmax-函数><a href=https://akzj.github.io/llm-basics-for-beginners/softmax/>Softmax 函数</a><a hidden class=anchor aria-hidden=true href=#softmax-函数>#</a></h4><p>Softmax 函数是 LLM 中用于将向量转换为概率分布的关键函数。</p><p><strong>学习内容</strong>：</p><ul><li>Softmax 函数的定义与特性</li><li>Softmax 在 LLM 中的应用</li><li>Softmax 的数值稳定性问题</li><li>Softmax 的实现示例</li></ul><h3 id=transformer-核心篇>Transformer 核心篇<a hidden class=anchor aria-hidden=true href=#transformer-核心篇>#</a></h3><p>（即将推出）</p><h4 id=attention-机制详解>Attention 机制详解<a hidden class=anchor aria-hidden=true href=#attention-机制详解>#</a></h4><p>深入理解 Transformer 中最核心的注意力机制</p><h4 id=位置编码>位置编码<a hidden class=anchor aria-hidden=true href=#位置编码>#</a></h4><p>学习 Transformer 如何处理序列的位置信息</p><h4 id=transformer-完整架构>Transformer 完整架构<a hidden class=anchor aria-hidden=true href=#transformer-完整架构>#</a></h4><p>全面掌握 Transformer 的编码器-解码器结构</p><h3 id=llm-进阶篇>LLM 进阶篇<a hidden class=anchor aria-hidden=true href=#llm-进阶篇>#</a></h3><p>（即将推出）</p><h4 id=模型训练与优化>模型训练与优化<a hidden class=anchor aria-hidden=true href=#模型训练与优化>#</a></h4><p>学习 LLM 的训练过程和优化技术</p><h4 id=模型评估与部署>模型评估与部署<a hidden class=anchor aria-hidden=true href=#模型评估与部署>#</a></h4><p>了解如何评估 LLM 的性能并进行部署</p><h4 id=最新研究进展>最新研究进展<a hidden class=anchor aria-hidden=true href=#最新研究进展>#</a></h4><p>跟踪 LLM 领域的最新研究成果</p><h2 id=学习路径建议>学习路径建议<a hidden class=anchor aria-hidden=true href=#学习路径建议>#</a></h2><p>对于初学者，我们建议按照以下顺序学习：</p><ol><li><strong>数学基础</strong>：先学习矩阵基础知识和 Softmax 函数，建立数学基础</li><li><strong>Transformer 核心</strong>：理解注意力机制和 Transformer 架构</li><li><strong>LLM 进阶</strong>：学习模型训练、评估和部署</li></ol><p>每个知识点都有详细的讲解和示例，帮助你循序渐进地掌握 LLM 的基础知识。</p><h2 id=关于本网站>关于本网站<a hidden class=anchor aria-hidden=true href=#关于本网站>#</a></h2><p>本网站旨在为初学者提供清晰、系统的 LLM 基础知识学习资源。我们将持续更新内容，涵盖更多 LLM 相关的知识点和技术。</p><h2 id=联系方式>联系方式<a hidden class=anchor aria-hidden=true href=#联系方式>#</a></h2><p>如有问题或建议，欢迎联系我们。</p></div></main><footer class=footer><span>&copy; 2025 <a href=https://akzj.github.io/llm-basics-for-beginners/>LLM Basics for Beginners</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script></body></html>