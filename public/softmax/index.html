<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>概率论与Softmax函数：深入理解与应用案例 | LLM Basics for Beginners</title>
<meta name=keywords content><meta name=description content="概率论与Softmax函数：深入理解与应用案例
1. 引言
在概率论和机器学习领域，Softmax函数是一个至关重要的概念。它不仅是连接线性模型和概率分布的桥梁，也是许多高级模型（如神经网络分类器）的核心组件。本文将深入探讨Softmax函数的数学原理、性质及其在实际问题中的应用。
2. Softmax函数的数学定义
2.1 基本定义
Softmax函数（也称为归一化指数函数）将一个实数向量转换为一个概率分布。对于输入向量 ( z = [z_1, z_2, &mldr;, z_K]^T )，Softmax函数的定义为：
$$\sigma(z)i = \frac{e^{z_i}}{\sum{j=1}^K e^{z_j}} \quad \text{for } i = 1, 2, &mldr;, K$$
其中，( K ) 是类别的数量，( \sigma(z)_i ) 表示样本属于第 ( i ) 类的概率。
2.2 直观理解
Softmax函数的核心思想是：

对每个输入值取指数，确保输出值非负
将所有指数结果求和，作为归一化因子
每个指数结果除以这个归一化因子，得到概率分布（所有概率之和为1）

3. Softmax函数的重要性质
3.1 归一化性质
Softmax函数的输出是一个有效的概率分布，满足：
$$\sum_{i=1}^K \sigma(z)_i = 1$$
这是由归一化因子的定义直接保证的。
3.2 平移不变性
Softmax函数具有平移不变性，即对输入向量的所有元素加上相同的常数，输出概率保持不变：
$$\sigma(z + c)_i = \sigma(z)_i \quad \text{for any constant } c$$"><meta name=author content><link rel=canonical href=https://akzj.github.io/llm-basics-for-beginners/softmax/><link crossorigin=anonymous href=/llm-basics-for-beginners/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css integrity="sha256-NDzEgLn/yPBMy+XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as=style><link rel=icon href=https://akzj.github.io/llm-basics-for-beginners/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://akzj.github.io/llm-basics-for-beginners/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://akzj.github.io/llm-basics-for-beginners/favicon-32x32.png><link rel=apple-touch-icon href=https://akzj.github.io/llm-basics-for-beginners/apple-touch-icon.png><link rel=mask-icon href=https://akzj.github.io/llm-basics-for-beginners/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://akzj.github.io/llm-basics-for-beginners/softmax/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=/llm-basics-for-beginners/css/custom.min.7d2ee8ecd3b435c2f737e79952e8b22b4022ee7ba372f20c8c44f3a2792a885f.css integrity="sha256-fS7o7NO0NcL3N+eZUuiyK0Ai7nujcvIMjETzonkqiF8="><script defer type=text/javascript id=MathJax-script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js?config=TeX-MML-AM_CHTML"></script><script defer type=text/javascript>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0,packages:["base","ams","noerrors","noundefined"]},options:{skipHtmlTags:["script","noscript","style","textarea","pre"],enableMenu:!1},loader:{load:["[tex]/ams","[tex]/noerrors","[tex]/noundefined"]}}</script><meta property="og:url" content="https://akzj.github.io/llm-basics-for-beginners/softmax/"><meta property="og:site_name" content="LLM Basics for Beginners"><meta property="og:title" content="概率论与Softmax函数：深入理解与应用案例"><meta property="og:description" content="概率论与Softmax函数：深入理解与应用案例 1. 引言 在概率论和机器学习领域，Softmax函数是一个至关重要的概念。它不仅是连接线性模型和概率分布的桥梁，也是许多高级模型（如神经网络分类器）的核心组件。本文将深入探讨Softmax函数的数学原理、性质及其在实际问题中的应用。
2. Softmax函数的数学定义 2.1 基本定义 Softmax函数（也称为归一化指数函数）将一个实数向量转换为一个概率分布。对于输入向量 ( z = [z_1, z_2, …, z_K]^T )，Softmax函数的定义为：
$$\sigma(z)i = \frac{e^{z_i}}{\sum{j=1}^K e^{z_j}} \quad \text{for } i = 1, 2, …, K$$
其中，( K ) 是类别的数量，( \sigma(z)_i ) 表示样本属于第 ( i ) 类的概率。
2.2 直观理解 Softmax函数的核心思想是：
对每个输入值取指数，确保输出值非负 将所有指数结果求和，作为归一化因子 每个指数结果除以这个归一化因子，得到概率分布（所有概率之和为1） 3. Softmax函数的重要性质 3.1 归一化性质 Softmax函数的输出是一个有效的概率分布，满足：
$$\sum_{i=1}^K \sigma(z)_i = 1$$
这是由归一化因子的定义直接保证的。
3.2 平移不变性 Softmax函数具有平移不变性，即对输入向量的所有元素加上相同的常数，输出概率保持不变：
$$\sigma(z + c)_i = \sigma(z)_i \quad \text{for any constant } c$$"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:published_time" content="2025-12-05T10:00:00+08:00"><meta property="article:modified_time" content="2025-12-05T10:00:00+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="概率论与Softmax函数：深入理解与应用案例"><meta name=twitter:description content="概率论与Softmax函数：深入理解与应用案例
1. 引言
在概率论和机器学习领域，Softmax函数是一个至关重要的概念。它不仅是连接线性模型和概率分布的桥梁，也是许多高级模型（如神经网络分类器）的核心组件。本文将深入探讨Softmax函数的数学原理、性质及其在实际问题中的应用。
2. Softmax函数的数学定义
2.1 基本定义
Softmax函数（也称为归一化指数函数）将一个实数向量转换为一个概率分布。对于输入向量 ( z = [z_1, z_2, &mldr;, z_K]^T )，Softmax函数的定义为：
$$\sigma(z)i = \frac{e^{z_i}}{\sum{j=1}^K e^{z_j}} \quad \text{for } i = 1, 2, &mldr;, K$$
其中，( K ) 是类别的数量，( \sigma(z)_i ) 表示样本属于第 ( i ) 类的概率。
2.2 直观理解
Softmax函数的核心思想是：

对每个输入值取指数，确保输出值非负
将所有指数结果求和，作为归一化因子
每个指数结果除以这个归一化因子，得到概率分布（所有概率之和为1）

3. Softmax函数的重要性质
3.1 归一化性质
Softmax函数的输出是一个有效的概率分布，满足：
$$\sum_{i=1}^K \sigma(z)_i = 1$$
这是由归一化因子的定义直接保证的。
3.2 平移不变性
Softmax函数具有平移不变性，即对输入向量的所有元素加上相同的常数，输出概率保持不变：
$$\sigma(z + c)_i = \sigma(z)_i \quad \text{for any constant } c$$"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"概率论与Softmax函数：深入理解与应用案例","item":"https://akzj.github.io/llm-basics-for-beginners/softmax/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"概率论与Softmax函数：深入理解与应用案例","name":"概率论与Softmax函数：深入理解与应用案例","description":"概率论与Softmax函数：深入理解与应用案例 1. 引言 在概率论和机器学习领域，Softmax函数是一个至关重要的概念。它不仅是连接线性模型和概率分布的桥梁，也是许多高级模型（如神经网络分类器）的核心组件。本文将深入探讨Softmax函数的数学原理、性质及其在实际问题中的应用。\n2. Softmax函数的数学定义 2.1 基本定义 Softmax函数（也称为归一化指数函数）将一个实数向量转换为一个概率分布。对于输入向量 ( z = [z_1, z_2, \u0026hellip;, z_K]^T )，Softmax函数的定义为：\n$$\\sigma(z)i = \\frac{e^{z_i}}{\\sum{j=1}^K e^{z_j}} \\quad \\text{for } i = 1, 2, \u0026hellip;, K$$\n其中，( K ) 是类别的数量，( \\sigma(z)_i ) 表示样本属于第 ( i ) 类的概率。\n2.2 直观理解 Softmax函数的核心思想是：\n对每个输入值取指数，确保输出值非负 将所有指数结果求和，作为归一化因子 每个指数结果除以这个归一化因子，得到概率分布（所有概率之和为1） 3. Softmax函数的重要性质 3.1 归一化性质 Softmax函数的输出是一个有效的概率分布，满足：\n$$\\sum_{i=1}^K \\sigma(z)_i = 1$$\n这是由归一化因子的定义直接保证的。\n3.2 平移不变性 Softmax函数具有平移不变性，即对输入向量的所有元素加上相同的常数，输出概率保持不变：\n$$\\sigma(z + c)_i = \\sigma(z)_i \\quad \\text{for any constant } c$$\n","keywords":[],"articleBody":"概率论与Softmax函数：深入理解与应用案例 1. 引言 在概率论和机器学习领域，Softmax函数是一个至关重要的概念。它不仅是连接线性模型和概率分布的桥梁，也是许多高级模型（如神经网络分类器）的核心组件。本文将深入探讨Softmax函数的数学原理、性质及其在实际问题中的应用。\n2. Softmax函数的数学定义 2.1 基本定义 Softmax函数（也称为归一化指数函数）将一个实数向量转换为一个概率分布。对于输入向量 ( z = [z_1, z_2, …, z_K]^T )，Softmax函数的定义为：\n$$\\sigma(z)i = \\frac{e^{z_i}}{\\sum{j=1}^K e^{z_j}} \\quad \\text{for } i = 1, 2, …, K$$\n其中，( K ) 是类别的数量，( \\sigma(z)_i ) 表示样本属于第 ( i ) 类的概率。\n2.2 直观理解 Softmax函数的核心思想是：\n对每个输入值取指数，确保输出值非负 将所有指数结果求和，作为归一化因子 每个指数结果除以这个归一化因子，得到概率分布（所有概率之和为1） 3. Softmax函数的重要性质 3.1 归一化性质 Softmax函数的输出是一个有效的概率分布，满足：\n$$\\sum_{i=1}^K \\sigma(z)_i = 1$$\n这是由归一化因子的定义直接保证的。\n3.2 平移不变性 Softmax函数具有平移不变性，即对输入向量的所有元素加上相同的常数，输出概率保持不变：\n$$\\sigma(z + c)_i = \\sigma(z)_i \\quad \\text{for any constant } c$$\n证明：\n$$\\sigma(z + c)i = \\frac{e^{z_i + c}}{\\sum{j=1}^K e^{z_j + c}} = \\frac{e^c e^{z_i}}{e^c \\sum_{j=1}^K e^{z_j}} = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}} = \\sigma(z)_i$$\n这个性质在实际计算中非常有用，可以通过减去输入向量的最大值来避免数值溢出问题。\n3.3 单调性 Softmax函数对于每个输入值是单调递增的：\n$$\\text{If } z_i \u003e z_j, \\text{ then } \\sigma(z)_i \u003e \\sigma(z)_j$$\n这意味着更高的输入值会产生更高的输出概率。\n4. 概率论视角下的Softmax函数 4.1 对数似然与交叉熵 在分类问题中，我们通常使用交叉熵损失函数来训练模型。对于Softmax输出和真实标签 ( y )（one-hot编码），交叉熵损失定义为：\n$$L(\\theta) = -\\sum_{i=1}^K y_i \\log(\\sigma(z)_i)$$\n其中，( \\theta ) 是模型参数。\n4.2 与最大似然估计的联系 使用交叉熵损失函数等价于最大化对数似然。对于独立同分布的样本，对数似然函数为：\n$$\\log P(Y|X; \\theta) = \\sum_{n=1}^N \\sum_{i=1}^K y_{n,i} \\log(\\sigma(z_n)_i)$$\n最小化交叉熵损失就是最大化这个对数似然函数。\n4.3 与熵的关系 Softmax函数的输出概率分布的熵可以表示为：\n$$H(\\sigma(z)) = -\\sum_{i=1}^K \\sigma(z)_i \\log(\\sigma(z)_i)$$\n熵衡量了概率分布的不确定性，当所有概率相等时熵最大，当只有一个类别概率为1时熵最小（为0）。\n5. Softmax函数的应用案例 5.1 多类别分类问题 假设我们有一个手写数字识别任务，需要将数字图像分为10个类别（0-9）。\n案例描述：\n输入：图像特征向量 ( x \\in \\mathbb{R}^{784} )（28x28像素） 模型：线性分类器 ( z = Wx + b )，其中 ( W \\in \\mathbb{R}^{10 \\times 784} )，( b \\in \\mathbb{R}^{10} ) 输出：( y = \\text{softmax}(z) ) 概率解释：\n输入图像经过线性变换得到10个得分值 ( z_0, z_1, …, z_9 ) Softmax函数将这些得分转换为10个概率值 ( P(Y=0|x), P(Y=1|x), …, P(Y=9|x) ) 模型预测概率最高的类别为最终结果 5.2 语言模型中的应用 在自然语言处理中，Softmax函数常用于语言模型中预测下一个单词的概率。\n案例描述：\n输入：前序单词的特征表示 模型：循环神经网络（RNN）或Transformer 输出：词汇表中所有单词的概率分布 概率解释：\n模型输出层产生每个单词的得分 Softmax函数将这些得分转换为概率分布 概率最高的单词被选为下一个最可能的单词 5.3 强化学习中的策略网络 在强化学习中，Softmax函数常用于策略网络中生成动作概率。\n案例描述：\n输入：环境状态的特征表示 模型：神经网络 输出：所有可能动作的概率分布 概率解释：\n模型输出层产生每个动作的得分 Softmax函数将这些得分转换为动作概率分布 智能体根据这个概率分布选择动作 6. 数值计算与实现技巧 6.1 避免数值溢出 当输入值 ( z_i ) 很大时，( e^{z_i} ) 可能会溢出。为了避免这个问题，可以利用Softmax的平移不变性：\n$$\\sigma(z)i = \\frac{e^{z_i - \\max(z)}}{\\sum{j=1}^K e^{z_j - \\max(z)}}$$\n其中，( \\max(z) ) 是输入向量的最大值。\n6.2 Python实现示例 1 2 3 4 5 6 7 8 9 10 11 12 13 import numpy as np def softmax(z): # 减去最大值以避免数值溢出 exp_z = np.exp(z - np.max(z)) return exp_z / exp_z.sum(axis=0) # 示例使用 z = np.array([2.0, 1.0, 0.1]) probabilities = softmax(z) print(\"输入向量:\", z) print(\"Softmax输出概率:\", probabilities) print(\"概率和:\", probabilities.sum()) 输出：\n1 2 3 输入向量: [2. 1. 0.1] Softmax输出概率: [0.65900114 0.24243297 0.09856589] 概率和: 1.0 7. Softmax函数的扩展 7.1 Temperature参数 可以通过引入温度参数 ( T ) 来调整Softmax的输出分布：\n$$\\sigma_T(z)i = \\frac{e^{z_i / T}}{\\sum{j=1}^K e^{z_j / T}}$$\n当 ( T \\to 0 ) 时，分布变得尖锐（趋向于one-hot编码） 当 ( T \\to \\infty ) 时，分布变得均匀 7.2 Gumbel-Softmax技巧 Gumbel-Softmax是Softmax的一个变体，用于在离散空间中进行可微分的采样：\n$$\\text{Gumbel-Softmax}(z)i = \\frac{e^{(z_i + g_i) / T}}{\\sum{j=1}^K e^{(z_j + g_j) / T}}$$\n其中，( g_i ) 是从Gumbel分布中采样的噪声。\n8. 总结 Softmax函数是概率论和机器学习中的一个关键概念，它将实数向量转换为概率分布，为分类问题和概率建模提供了基础。本文从数学定义、重要性质、概率论视角和应用案例等多个方面对Softmax函数进行了深入讲解。\n通过理解Softmax函数，我们可以更好地掌握机器学习中的分类模型、概率建模和决策过程，为进一步学习更复杂的模型奠定基础。\n9. 练习题 计算输入向量 ( z = [1.0, 2.0, 3.0] ) 的Softmax输出。 证明Softmax函数的归一化性质。 解释为什么Softmax函数在多类别分类问题中如此重要。 实现带温度参数的Softmax函数，并观察不同温度值对输出分布的影响。 思考Softmax函数与其他归一化方法（如Sigmoid）的区别和联系。 ","wordCount":"375","inLanguage":"en","datePublished":"2025-12-05T10:00:00+08:00","dateModified":"2025-12-05T10:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://akzj.github.io/llm-basics-for-beginners/softmax/"},"publisher":{"@type":"Organization","name":"LLM Basics for Beginners","logo":{"@type":"ImageObject","url":"https://akzj.github.io/llm-basics-for-beginners/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://akzj.github.io/llm-basics-for-beginners/ accesskey=h title="LLM Basics for Beginners (Alt + H)">LLM Basics for Beginners</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://akzj.github.io/llm-basics-for-beginners/ title=首页><span>首页</span></a></li><li><a href=https://akzj.github.io/llm-basics-for-beginners/matrix_basics/ title=矩阵基础><span>矩阵基础</span></a></li><li><a href=https://akzj.github.io/llm-basics-for-beginners/about/ title=关于><span>关于</span></a></li><li><a href=https://akzj.github.io/llm-basics-for-beginners/softmax/ title=Softmax函数><span class=active>Softmax函数</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">概率论与Softmax函数：深入理解与应用案例</h1><div class=post-meta><span title='2025-12-05 10:00:00 +0800 CST'>December 5, 2025</span></div></header><div class=post-content><h1 id=概率论与softmax函数深入理解与应用案例>概率论与Softmax函数：深入理解与应用案例<a hidden class=anchor aria-hidden=true href=#概率论与softmax函数深入理解与应用案例>#</a></h1><h2 id=1-引言>1. 引言<a hidden class=anchor aria-hidden=true href=#1-引言>#</a></h2><p>在概率论和机器学习领域，Softmax函数是一个至关重要的概念。它不仅是连接线性模型和概率分布的桥梁，也是许多高级模型（如神经网络分类器）的核心组件。本文将深入探讨Softmax函数的数学原理、性质及其在实际问题中的应用。</p><h2 id=2-softmax函数的数学定义>2. Softmax函数的数学定义<a hidden class=anchor aria-hidden=true href=#2-softmax函数的数学定义>#</a></h2><h3 id=21-基本定义>2.1 基本定义<a hidden class=anchor aria-hidden=true href=#21-基本定义>#</a></h3><p>Softmax函数（也称为归一化指数函数）将一个实数向量转换为一个概率分布。对于输入向量 ( z = [z_1, z_2, &mldr;, z_K]^T )，Softmax函数的定义为：</p><p>$$\sigma(z)<em>i = \frac{e^{z_i}}{\sum</em>{j=1}^K e^{z_j}} \quad \text{for } i = 1, 2, &mldr;, K$$</p><p>其中，( K ) 是类别的数量，( \sigma(z)_i ) 表示样本属于第 ( i ) 类的概率。</p><h3 id=22-直观理解>2.2 直观理解<a hidden class=anchor aria-hidden=true href=#22-直观理解>#</a></h3><p>Softmax函数的核心思想是：</p><ol><li>对每个输入值取指数，确保输出值非负</li><li>将所有指数结果求和，作为归一化因子</li><li>每个指数结果除以这个归一化因子，得到概率分布（所有概率之和为1）</li></ol><h2 id=3-softmax函数的重要性质>3. Softmax函数的重要性质<a hidden class=anchor aria-hidden=true href=#3-softmax函数的重要性质>#</a></h2><h3 id=31-归一化性质>3.1 归一化性质<a hidden class=anchor aria-hidden=true href=#31-归一化性质>#</a></h3><p>Softmax函数的输出是一个有效的概率分布，满足：</p><p>$$\sum_{i=1}^K \sigma(z)_i = 1$$</p><p>这是由归一化因子的定义直接保证的。</p><h3 id=32-平移不变性>3.2 平移不变性<a hidden class=anchor aria-hidden=true href=#32-平移不变性>#</a></h3><p>Softmax函数具有平移不变性，即对输入向量的所有元素加上相同的常数，输出概率保持不变：</p><p>$$\sigma(z + c)_i = \sigma(z)_i \quad \text{for any constant } c$$</p><p>证明：</p><p>$$\sigma(z + c)<em>i = \frac{e^{z_i + c}}{\sum</em>{j=1}^K e^{z_j + c}} = \frac{e^c e^{z_i}}{e^c \sum_{j=1}^K e^{z_j}} = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}} = \sigma(z)_i$$</p><p>这个性质在实际计算中非常有用，可以通过减去输入向量的最大值来避免数值溢出问题。</p><h3 id=33-单调性>3.3 单调性<a hidden class=anchor aria-hidden=true href=#33-单调性>#</a></h3><p>Softmax函数对于每个输入值是单调递增的：</p><p>$$\text{If } z_i > z_j, \text{ then } \sigma(z)_i > \sigma(z)_j$$</p><p>这意味着更高的输入值会产生更高的输出概率。</p><h2 id=4-概率论视角下的softmax函数>4. 概率论视角下的Softmax函数<a hidden class=anchor aria-hidden=true href=#4-概率论视角下的softmax函数>#</a></h2><h3 id=41-对数似然与交叉熵>4.1 对数似然与交叉熵<a hidden class=anchor aria-hidden=true href=#41-对数似然与交叉熵>#</a></h3><p>在分类问题中，我们通常使用交叉熵损失函数来训练模型。对于Softmax输出和真实标签 ( y )（one-hot编码），交叉熵损失定义为：</p><p>$$L(\theta) = -\sum_{i=1}^K y_i \log(\sigma(z)_i)$$</p><p>其中，( \theta ) 是模型参数。</p><h3 id=42-与最大似然估计的联系>4.2 与最大似然估计的联系<a hidden class=anchor aria-hidden=true href=#42-与最大似然估计的联系>#</a></h3><p>使用交叉熵损失函数等价于最大化对数似然。对于独立同分布的样本，对数似然函数为：</p><p>$$\log P(Y|X; \theta) = \sum_{n=1}^N \sum_{i=1}^K y_{n,i} \log(\sigma(z_n)_i)$$</p><p>最小化交叉熵损失就是最大化这个对数似然函数。</p><h3 id=43-与熵的关系>4.3 与熵的关系<a hidden class=anchor aria-hidden=true href=#43-与熵的关系>#</a></h3><p>Softmax函数的输出概率分布的熵可以表示为：</p><p>$$H(\sigma(z)) = -\sum_{i=1}^K \sigma(z)_i \log(\sigma(z)_i)$$</p><p>熵衡量了概率分布的不确定性，当所有概率相等时熵最大，当只有一个类别概率为1时熵最小（为0）。</p><h2 id=5-softmax函数的应用案例>5. Softmax函数的应用案例<a hidden class=anchor aria-hidden=true href=#5-softmax函数的应用案例>#</a></h2><h3 id=51-多类别分类问题>5.1 多类别分类问题<a hidden class=anchor aria-hidden=true href=#51-多类别分类问题>#</a></h3><p>假设我们有一个手写数字识别任务，需要将数字图像分为10个类别（0-9）。</p><p><strong>案例描述</strong>：</p><ul><li>输入：图像特征向量 ( x \in \mathbb{R}^{784} )（28x28像素）</li><li>模型：线性分类器 ( z = Wx + b )，其中 ( W \in \mathbb{R}^{10 \times 784} )，( b \in \mathbb{R}^{10} )</li><li>输出：( y = \text{softmax}(z) )</li></ul><p><strong>概率解释</strong>：</p><ul><li>输入图像经过线性变换得到10个得分值 ( z_0, z_1, &mldr;, z_9 )</li><li>Softmax函数将这些得分转换为10个概率值 ( P(Y=0|x), P(Y=1|x), &mldr;, P(Y=9|x) )</li><li>模型预测概率最高的类别为最终结果</li></ul><h3 id=52-语言模型中的应用>5.2 语言模型中的应用<a hidden class=anchor aria-hidden=true href=#52-语言模型中的应用>#</a></h3><p>在自然语言处理中，Softmax函数常用于语言模型中预测下一个单词的概率。</p><p><strong>案例描述</strong>：</p><ul><li>输入：前序单词的特征表示</li><li>模型：循环神经网络（RNN）或Transformer</li><li>输出：词汇表中所有单词的概率分布</li></ul><p><strong>概率解释</strong>：</p><ul><li>模型输出层产生每个单词的得分</li><li>Softmax函数将这些得分转换为概率分布</li><li>概率最高的单词被选为下一个最可能的单词</li></ul><h3 id=53-强化学习中的策略网络>5.3 强化学习中的策略网络<a hidden class=anchor aria-hidden=true href=#53-强化学习中的策略网络>#</a></h3><p>在强化学习中，Softmax函数常用于策略网络中生成动作概率。</p><p><strong>案例描述</strong>：</p><ul><li>输入：环境状态的特征表示</li><li>模型：神经网络</li><li>输出：所有可能动作的概率分布</li></ul><p><strong>概率解释</strong>：</p><ul><li>模型输出层产生每个动作的得分</li><li>Softmax函数将这些得分转换为动作概率分布</li><li>智能体根据这个概率分布选择动作</li></ul><h2 id=6-数值计算与实现技巧>6. 数值计算与实现技巧<a hidden class=anchor aria-hidden=true href=#6-数值计算与实现技巧>#</a></h2><h3 id=61-避免数值溢出>6.1 避免数值溢出<a hidden class=anchor aria-hidden=true href=#61-避免数值溢出>#</a></h3><p>当输入值 ( z_i ) 很大时，( e^{z_i} ) 可能会溢出。为了避免这个问题，可以利用Softmax的平移不变性：</p><p>$$\sigma(z)<em>i = \frac{e^{z_i - \max(z)}}{\sum</em>{j=1}^K e^{z_j - \max(z)}}$$</p><p>其中，( \max(z) ) 是输入向量的最大值。</p><h3 id=62-python实现示例>6.2 Python实现示例<a hidden class=anchor aria-hidden=true href=#62-python实现示例>#</a></h3><div class=highlight><div style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#737679"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#737679"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#737679"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#737679"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#737679"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#737679"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#737679"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#737679"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#737679"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#737679">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#737679">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#737679">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#737679">13
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff7b72>import</span> <span style=color:#ff7b72>numpy</span> <span style=color:#ff7b72>as</span> <span style=color:#ff7b72>np</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff7b72>def</span> <span style=color:#d2a8ff;font-weight:700>softmax</span>(z):
</span></span><span style=display:flex><span>    <span style=color:#8b949e;font-style:italic># 减去最大值以避免数值溢出</span>
</span></span><span style=display:flex><span>    exp_z <span style=color:#ff7b72;font-weight:700>=</span> np<span style=color:#ff7b72;font-weight:700>.</span>exp(z <span style=color:#ff7b72;font-weight:700>-</span> np<span style=color:#ff7b72;font-weight:700>.</span>max(z))
</span></span><span style=display:flex><span>    <span style=color:#ff7b72>return</span> exp_z <span style=color:#ff7b72;font-weight:700>/</span> exp_z<span style=color:#ff7b72;font-weight:700>.</span>sum(axis<span style=color:#ff7b72;font-weight:700>=</span><span style=color:#a5d6ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8b949e;font-style:italic># 示例使用</span>
</span></span><span style=display:flex><span>z <span style=color:#ff7b72;font-weight:700>=</span> np<span style=color:#ff7b72;font-weight:700>.</span>array([<span style=color:#a5d6ff>2.0</span>, <span style=color:#a5d6ff>1.0</span>, <span style=color:#a5d6ff>0.1</span>])
</span></span><span style=display:flex><span>probabilities <span style=color:#ff7b72;font-weight:700>=</span> softmax(z)
</span></span><span style=display:flex><span>print(<span style=color:#a5d6ff>&#34;输入向量:&#34;</span>, z)
</span></span><span style=display:flex><span>print(<span style=color:#a5d6ff>&#34;Softmax输出概率:&#34;</span>, probabilities)
</span></span><span style=display:flex><span>print(<span style=color:#a5d6ff>&#34;概率和:&#34;</span>, probabilities<span style=color:#ff7b72;font-weight:700>.</span>sum())
</span></span></code></pre></td></tr></table></div></div><p>输出：</p><div class=highlight><div style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#737679">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#737679">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#737679">3
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>输入向量: [2.  1.  0.1]
</span></span><span style=display:flex><span>Softmax输出概率: [0.65900114 0.24243297 0.09856589]
</span></span><span style=display:flex><span>概率和: 1.0
</span></span></code></pre></td></tr></table></div></div><h2 id=7-softmax函数的扩展>7. Softmax函数的扩展<a hidden class=anchor aria-hidden=true href=#7-softmax函数的扩展>#</a></h2><h3 id=71-temperature参数>7.1 Temperature参数<a hidden class=anchor aria-hidden=true href=#71-temperature参数>#</a></h3><p>可以通过引入温度参数 ( T ) 来调整Softmax的输出分布：</p><p>$$\sigma_T(z)<em>i = \frac{e^{z_i / T}}{\sum</em>{j=1}^K e^{z_j / T}}$$</p><ul><li>当 ( T \to 0 ) 时，分布变得尖锐（趋向于one-hot编码）</li><li>当 ( T \to \infty ) 时，分布变得均匀</li></ul><h3 id=72-gumbel-softmax技巧>7.2 Gumbel-Softmax技巧<a hidden class=anchor aria-hidden=true href=#72-gumbel-softmax技巧>#</a></h3><p>Gumbel-Softmax是Softmax的一个变体，用于在离散空间中进行可微分的采样：</p><p>$$\text{Gumbel-Softmax}(z)<em>i = \frac{e^{(z_i + g_i) / T}}{\sum</em>{j=1}^K e^{(z_j + g_j) / T}}$$</p><p>其中，( g_i ) 是从Gumbel分布中采样的噪声。</p><h2 id=8-总结>8. 总结<a hidden class=anchor aria-hidden=true href=#8-总结>#</a></h2><p>Softmax函数是概率论和机器学习中的一个关键概念，它将实数向量转换为概率分布，为分类问题和概率建模提供了基础。本文从数学定义、重要性质、概率论视角和应用案例等多个方面对Softmax函数进行了深入讲解。</p><p>通过理解Softmax函数，我们可以更好地掌握机器学习中的分类模型、概率建模和决策过程，为进一步学习更复杂的模型奠定基础。</p><h2 id=9-练习题>9. 练习题<a hidden class=anchor aria-hidden=true href=#9-练习题>#</a></h2><ol><li>计算输入向量 ( z = [1.0, 2.0, 3.0] ) 的Softmax输出。</li><li>证明Softmax函数的归一化性质。</li><li>解释为什么Softmax函数在多类别分类问题中如此重要。</li><li>实现带温度参数的Softmax函数，并观察不同温度值对输出分布的影响。</li><li>思考Softmax函数与其他归一化方法（如Sigmoid）的区别和联系。</li></ol></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://akzj.github.io/llm-basics-for-beginners/>LLM Basics for Beginners</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script></body></html>