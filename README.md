# llm-basics-for-beginners


### 一、Transformer论文核心公式+对应数学知识点清单（精准对接，看懂就够）
| 论文核心公式/模块       | 关键数学知识点                | 核心理解（不用深钻证明，懂应用）                                                                 |
|-------------------------|-----------------------------|--------------------------------------------------------------------------------------------------|
| 1. 注意力机制核心公式<br>$Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$ | 线性代数：矩阵乘法、向量内积；<br>概率论：softmax函数 | - $QK^T$：矩阵乘法，本质是计算查询（Q）与键（K）的相似度（内积越大，相似度越高）；<br>- $\sqrt{d_k}$：缩放因子，避免内积过大导致softmax梯度消失；<br>- softmax：将相似度转化为0-1的概率权重，实现“关注重要信息”。 |
| 2. 多头注意力<br>$MultiHead(Q,K,V) = Concat(head_1,...,head_h)W^O$<br>其中 $head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$ | 线性代数：矩阵投影（$W_i^Q$等权重矩阵）、矩阵拼接（Concat） | - 多头=多组“Q/K/V投影”，每组聚焦不同维度的信息（比如一组关注语法，一组关注语义）；<br>- 最后拼接后乘$W^O$，是将多组信息融合为统一维度。 |
| 3. 位置编码<br>$PE(pos,2i) = sin(pos/10000^{2i/d_{model}})$<br>$PE(pos,2i+1) = cos(pos/10000^{2i/d_{model}})$ | 三角函数：sin/cos周期性；<br>线性代数：位置向量拼接 | - 利用三角函数的周期性，给每个位置分配唯一向量，让模型感知“语序”（Transformer本身无顺序信息）；<br>- 不同频率的sin/cos对应不同位置尺度，兼顾长短期依赖。 |
| 4. 层归一化（LayerNorm）<br>$LN(x) = \gamma \cdot \frac{x-\mu}{\sqrt{\sigma^2+\epsilon}} + \beta$ | 线性代数：向量均值/方差计算；<br>数值优化：归一化逻辑 | - $\mu$（均值）、$\sigma^2$（方差）：对层输出向量做标准化，避免梯度消失；<br>- $\gamma$（缩放）、$\beta$（偏移）：可学习参数，保留模型灵活性。 |
| 5. 前馈网络（FFN）<br>$FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$ | 线性代数：矩阵乘法、偏置；<br>非线性激活：ReLU函数 | - $xW_1 + b_1$：线性变换（升维）；<br>- $max(0,x)$：ReLU激活，引入非线性，让模型学习复杂映射；<br>- 再通过$W_2$降维，输出最终特征。 |
| 6. 交叉熵损失函数<br>$Loss = -\sum y_{true} \cdot log(y_{pred})$ | 信息论：交叉熵；<br>概率论：对数概率 | - 衡量模型预测分布（$y_{pred}$）与真实分布（$y_{true}$）的差异，损失越小，预测越准；<br>- 本质是“惩罚预测错误的概率”（比如真实标签概率0.9，预测0.1，损失会很大）。 |
| 7. 反向传播与参数更新<br>$\theta = \theta - \eta \cdot \nabla Loss(\theta)$ | 最优化：梯度下降、偏导数；<br>链式法则 | - $\nabla Loss(\theta)$：损失函数对参数$\theta$的偏导数（梯度），指示参数更新方向；<br>- $\eta$（学习率）：控制更新步长，避免过冲或收敛过慢；<br>- 链式法则：是计算多层网络梯度的核心（比如FFN+注意力层的梯度传递）。 |

### 二、30天数学补完计划（每日详细任务表，每天1-1.5小时）
#### 第一阶段：线性代数（Transformer核心，10天）
| 天数 | 核心任务                          | 学习资源                          | 实操要求（必须完成，不然白学）                                                                 |
|------|-----------------------------------|-----------------------------------|------------------------------------------------------------------------------------------------|
| 1    | 向量、内积、矩阵定义与运算        | 3Blue1Brown《线性代数的本质》P1-P2；李永乐《线性代数辅导讲义》P1-P15 | 1. 看懂“向量是有方向的量”“内积是相似度度量”；<br>2. 手动计算2个2维向量的内积、2个2×2矩阵的乘法。 |
| 2    | 矩阵转置、逆矩阵、秩              | 李永乐讲义P16-P25；B站“宋浩线性代数”矩阵运算章节 | 1. 记住矩阵转置规则（$(AB)^T = B^T A^T$）；<br>2. 知道“秩”是矩阵的有效信息维度（不用会计算高维秩）。 |
| 3    | 线性方程组与向量空间              | 3Blue1Brown P3；李永乐讲义P26-P35 | 1. 理解“线性方程组的解就是向量空间中的交点”；<br>2. 能看懂论文中“Q/K/V是向量空间中的投影”。 |
| 4    | 特征值与特征向量                  | 3Blue1Brown P4；李永乐讲义P36-P45 | 1. 知道“特征向量是矩阵变换后方向不变的向量”；<br>2. 不用会复杂求解，能识别论文中特征值相关符号即可。 |
| 5    | SVD奇异值分解（重点！）           | 3Blue1Brown P5；知乎“SVD分解通俗解读” | 1. 理解“SVD能将矩阵拆分为3个简单矩阵，用于降维或信息提取”；<br>2. 知道注意力层中SVD可用于优化计算（比如减少参数）。 |
| 6    | 矩阵投影与权重矩阵                | 李永乐讲义P46-P55；Transformer多头注意力解读文章 | 1. 看懂“$QW_i^Q$是将Q投影到新的向量空间”；<br>2. 理解“权重矩阵是模型学习到的特征映射规则”。 |
| 7    | 向量拼接（Concat）与维度变换      | 李永乐讲义P56-P60；NumPy文档Concat函数 | 1. 知道“多头注意力拼接是将多组向量按列拼接”；<br>2. 用NumPy实现2个向量的拼接（比如[1,2]和[3,4]拼接为[1,2,3,4]）。 |
| 8    | 向量均值、方差计算                | 李永乐讲义P61-P65；Excel/NumPy计算均值方差 | 1. 会手动计算1组向量的均值和方差；<br>2. 理解层归一化中“均值方差是对每个样本的向量计算”。 |
| 9    | 矩阵微分初步（梯度计算基础）      | 花书第4章；B站“李宏毅矩阵微分” | 1. 记住“矩阵对向量的导数是雅可比矩阵”（不用深钻）；<br>2. 能看懂“损失函数对权重矩阵的梯度是$\nabla W = X^T \cdot error$”。 |
| 10   | 线性代数综合：拆解注意力核心公式  | 《Attention Is All You Need》原文公式1 | 1. 对着公式$Attention(Q,K,V)$，逐步写出矩阵运算过程；<br>2. 用NumPy实现简单的注意力计算（Q/K/V为2×2矩阵）。 |

#### 第二阶段：概率论+信息论（辅助理解模型设计，10天）
| 天数 | 核心任务                          | 学习资源                          | 实操要求                                                                 |
|------|-----------------------------------|-----------------------------------|--------------------------------------------------------------------------|
| 11   | 概率基础、古典概型、条件概率      | 茆诗松《概率论与数理统计》P1-P20；B站“宋浩概率论” | 1. 理解“概率是事件发生的可能性”；<br>2. 会计算简单的条件概率（比如“已知A发生，B发生的概率”）。 |
| 12   | 随机变量、概率分布（正态分布）    | 茆诗松P21-P40；3Blue1Brown《概率的本质》 | 1. 知道“正态分布是连续型随机变量的常见分布”；<br>2. 理解“模型输出的概率分布多符合正态分布”。 |
| 13   | softmax函数（重点！）             | 花书第3章；知乎“softmax函数通俗解读” | 1. 手动计算3个数值的softmax（比如[1,2,3]）；<br>2. 理解“softmax将任意数值转化为概率和为1的权重”。 |
| 14   | 熵与不确定性                      | 知乎“信息论入门：熵是什么”；B站“李永乐信息论” | 1. 记住“熵越大，不确定性越高”（比如均匀分布熵最大）；<br>2. 理解“模型训练是降低预测结果的熵”。 |
| 15   | KL散度（相对熵）                  | 花书第11章；信息论通俗文章         | 1. 知道“KL散度衡量两个分布的差异”（KL=0表示分布完全一致）；<br>2. 理解“生成模型中用KL散度约束分布相似”。 |
| 16   | 交叉熵损失函数（重点！）          | 花书第6章；B站“李宏毅交叉熵”     | 1. 手动计算简单的交叉熵（比如真实标签[1,0]，预测[0.9,0.1]）；<br>2. 理解“交叉熵越小，模型预测越准”。 |
| 17   | 最大似然估计                      | 茆诗松P100-P110；机器学习通俗文章 | 1. 知道“最大似然估计是找最可能产生观测数据的参数”；<br>2. 理解“模型训练本质是最大化似然函数”。 |
| 18   | 贝叶斯公式初步                    | 茆诗松P41-P50；知乎“贝叶斯公式应用” | 1. 记住贝叶斯公式$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$；<br>2. 理解“贝叶斯思维是‘根据新数据更新先验概率’”。 |
| 19   | 概率论综合：拆解softmax+交叉熵    | Transformer损失函数部分            | 1. 对着论文损失函数公式，理解“softmax输出概率后，用交叉熵计算损失”；<br>2. 用Python计算一组简单的损失值。 |
| 20   | 位置编码中的三角函数              | 高中三角函数复习；B站“三角函数周期性” | 1. 理解“sin/cos的周期性的用于表示位置顺序”；<br>2. 用Excel绘制sin(10000^(2i/d_model))的曲线，观察周期性。 |

#### 第三阶段：最优化基础（理解模型训练，10天）
| 天数 | 核心任务                          | 学习资源                          | 实操要求                                                                 |
|------|-----------------------------------|-----------------------------------|--------------------------------------------------------------------------|
| 21   | 偏导数、全微分                    | 宋浩《高等数学》偏导数章节；花书第4章 | 1. 会计算简单多元函数的偏导数（比如$z=x^2+2xy$对x的偏导）；<br>2. 理解“偏导数是单一变量的变化率”。 |
| 22   | 链式法则（重点！）                | 3Blue1Brown《微积分的本质》链式法则；宋浩高数 | 1. 手动推导复合函数的偏导数（比如$z=f(u,v)$，$u=g(x)$，$v=h(x)$）；<br>2. 理解“链式法则是反向传播的核心”。 |
| 23   | 梯度与梯度下降                    | 李宏毅《机器学习》梯度下降章节；花书第4章 | 1. 知道“梯度是偏导数组成的向量，指示函数变化最快的方向”；<br>2. 手动推导线性回归的梯度下降公式。 |
| 24   | 学习率与参数更新                  | 花书第8章；知乎“学习率怎么选”     | 1. 理解“学习率过大导致震荡，过小导致收敛慢”；<br>2. 用Python实现简单的梯度下降（比如优化函数$y=x^2$）。 |
| 25   | 动量、Adam优化器（重点！）        | 花书第8章；B站“Adam优化器通俗解读” | 1. 知道“Adam是‘梯度下降+动量+自适应学习率’的结合”；<br>2. 理解“Adam能加速模型收敛，避免局部最优”。 |
| 26   | 反向传播原理                      | 李宏毅《机器学习》反向传播章节；花书第6章 | 1. 看懂“反向传播是从损失函数反向计算每层的梯度”；<br>2. 手动推导2层神经网络的反向传播过程。 |
| 27   | 过拟合与正则化                    | 花书第7章；知乎“正则化是什么”     | 1. 知道“正则化是通过惩罚参数大小避免过拟合”；<br>2. 理解“L2正则化等价于给损失函数加参数的平方和”。 |
| 28   | 最优化综合：拆解Transformer训练流程 | Transformer训练部分原文            | 1. 梳理“前向传播（计算输出）→ 计算损失 → 反向传播（求梯度）→ 参数更新”的完整流程；<br>2. 标注每个步骤对应的数学知识点（比如前向传播用矩阵运算，反向传播用链式法则）。 |
| 29   | 论文公式整体拆解                  | 《Attention Is All You Need》原文公式1-6 | 1. 对着论文中的核心公式，逐一对应到前面学的数学知识点；<br>2. 能口头讲解每个公式的“数学逻辑+模型意义”。 |
| 30   | 实操：用NumPy实现简单注意力层     | 掘金“NumPy实现注意力机制”教程     | 1. 编写代码实现Q/K/V的矩阵运算、softmax、权重融合；<br>2. 运行代码，观察输入输出的维度变化，验证公式正确性。 |

### 核心提醒：
1. 不用追求“全对”，每天完成“实操要求”即可，比如手动计算、简单编程实现——数学只有落地才会真正记住；
2. 遇到不懂的知识点，先跳过复杂证明，只看“应用层面的解读”（比如SVD不用会推导，知道它能降维就行）；
3. 30天后再回头看Transformer论文，会发现公式都能对应到学过的知识点，成就感直接拉满～

## 三、扩展阅读

- [矩阵基础知识](./matrix_basics.md)：详细介绍LLM中常用的矩阵运算、概念和应用

需要我帮你整理一份“NumPy实现注意力层的极简代码模板”，或者标注论文中“必须看懂的核心公式位置”吗？